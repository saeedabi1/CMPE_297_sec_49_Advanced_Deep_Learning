{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 6_Extra_Credit_mnist_TFX_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6q0nlvwCKKw"
      },
      "source": [
        "# **Assignment 6_Extra_Credit_mnist_TFX_pipeline**\n",
        "\n",
        "a colab entirely in tfx interactive context for doing any of the following end2end with all artifacts for mnist dataset\n",
        "\n",
        "Reference: https://github.com/tensorflow/tfx/tree/master/tfx/examples/mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbC-BXECRJzt",
        "outputId": "a2f6a5de-5564-487f-8154-3c51bfa7ad50"
      },
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/11/2dc62c5263d9eb322f2f028f7b56cd9d096bb8988fcf82d65fa2e4057afe/pip-20.3.1-py2.py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 33.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 37.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 34.0MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 35.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 37.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 28.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 25.4MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 27.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 23.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 23.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 23.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 23.9MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 23.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 23.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 23.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 23.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 23.9MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 23.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 23.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 23.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 368kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 389kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 399kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 532kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 542kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 552kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 563kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 583kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 593kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 604kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 727kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 737kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 747kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 768kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 778kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 788kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 798kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 890kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 911kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 921kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 931kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 942kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 952kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 962kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 972kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 983kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 23.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UC6bObFRwCc"
      },
      "source": [
        "## **First we need to Install TFX to do this project:**\n",
        "The runtime should be restarted if we run this cell first time- if not this may show an error!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp9dvBrhRFVV",
        "outputId": "029a965d-17dd-4a38-b502-a61953109298"
      },
      "source": [
        "!pip install -q -U --use-feature=2020-resolver tfx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyZjjdSCFl6"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from typing import List, Text\n",
        "\n",
        "import absl\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiZymKs6STMX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tfx"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG4OQyQ8QzRS"
      },
      "source": [
        "import tensorflow_model_analysis as tfma"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjieUls1Qvk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e114b5e3-41e7-4eb8-8320-0b1feefb775c"
      },
      "source": [
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import ImportExampleGen\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.utils.dsl_utils import external_input"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLj0fSlGSK8N",
        "outputId": "f312c83d-5103-4ae1-8528-402c3c16d74f"
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n",
            "TFX version: 0.25.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPNSB6nRQney"
      },
      "source": [
        "_pipeline_name = 'mnist_native_keras'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv3sPxGNWcLj",
        "outputId": "24f9634f-4d5c-4ccb-ff1c-a778954b8b01"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWkvfbhYWcRf"
      },
      "source": [
        "path_to_mnist = '/content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/examples'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqNKmiiR7za"
      },
      "source": [
        "# This example assumes that MNIST data is stored in ~/mnist/data and the utility\n",
        "# function is in ~/mnist. Feel free to customize as needed.\n",
        "_mnist_root = os.path.join(path_to_mnist, 'mnist')\n",
        "_data_root = os.path.join(_mnist_root, 'data')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_kbT5ASR5XR"
      },
      "source": [
        "# Python module files to inject customized logic into the TFX components. The\n",
        "# Transform and Trainer both require user-defined functions to run successfully.\n",
        "_module_file = os.path.join(_mnist_root, 'mnist_utils_native_keras.py')\n",
        "_module_file_lite = os.path.join(\n",
        "    _mnist_root, 'mnist_utils_native_keras_lite.py')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-0NwnxNR38S"
      },
      "source": [
        "# Path which can be listened to by the model server. Pusher will output the\n",
        "# trained model here.\n",
        "_serving_model_dir = os.path.join(_mnist_root, 'serving_model', _pipeline_name)\n",
        "_serving_model_dir_lite = os.path.join(\n",
        "    _mnist_root, 'serving_model_lite', _pipeline_name)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5sa6K4sjzxW"
      },
      "source": [
        "path_to_tfx = '/content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZQyKM93R2Yu"
      },
      "source": [
        "# Directory and data locations.  This example assumes all of the images,\n",
        "# example code, and metadata library is relative to $HOME, but you can store\n",
        "# these files anywhere on your local filesystem.\n",
        "_tfx_root = os.path.join(path_to_tfx, 'tfx')\n",
        "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
        "# Sqlite ML-metadata db path.\n",
        "_metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name,\n",
        "                              'metadata.db')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sazF2GS2R0Wf"
      },
      "source": [
        "# Pipeline arguments for Beam powered Components.\n",
        "_beam_pipeline_args = [\n",
        "    '--direct_running_mode=multi_processing',\n",
        "    # 0 means auto-detect based on on the number of CPUs available\n",
        "    # during execution time.\n",
        "    '--direct_num_workers=0',\n",
        "]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MFmDlAyQctJ"
      },
      "source": [
        "def _create_pipeline(pipeline_name: Text, pipeline_root: Text, data_root: Text,\n",
        "                     module_file: Text, module_file_lite: Text,\n",
        "                     serving_model_dir: Text, serving_model_dir_lite: Text,\n",
        "                     metadata_path: Text,\n",
        "                     beam_pipeline_args: List[Text]) -> pipeline.Pipeline:\n",
        "  \"\"\"Implements the handwritten digit classification example using TFX.\"\"\"\n",
        "  examples = external_input(data_root)\n",
        "\n",
        "  # Brings data into the pipeline.\n",
        "  example_gen = ImportExampleGen(input=examples)\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # Generates schema based on statistics files.\n",
        "  schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "\n",
        "  # Performs anomaly detection based on statistics and data schema.\n",
        "  example_validator = ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema'])\n",
        "\n",
        "  # Performs transformations and feature engineering in training and serving.\n",
        "  transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      module_file=module_file)\n",
        "  def _create_trainer(module_file, instance_name):\n",
        "    return Trainer(\n",
        "        module_file=module_file,\n",
        "        custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=5000),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=100),\n",
        "        instance_name=instance_name)\n",
        "\n",
        "  # Uses user-provided Python function that trains a Keras model.\n",
        "  trainer = _create_trainer(module_file, 'mnist')\n",
        "\n",
        "  # Trains the same model as the one above, but converts it into a TFLite one.\n",
        "  trainer_lite = _create_trainer(module_file_lite, 'mnist_lite')\n",
        "\n",
        "  # TODO(b/150949276): Add resolver back once it supports two trainers.\n",
        "\n",
        "  # Uses TFMA to compute an evaluation statistics over features of a model and\n",
        "  # performs quality validation of a candidate model.\n",
        "  eval_config = tfma.EvalConfig(\n",
        "      model_specs=[tfma.ModelSpec(label_key='image_class')],\n",
        "      slicing_specs=[tfma.SlicingSpec()],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(metrics=[\n",
        "              tfma.MetricConfig(\n",
        "                  class_name='SparseCategoricalAccuracy',\n",
        "                  threshold=tfma.config.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.8})))\n",
        "          ])\n",
        "      ])\n",
        "\n",
        "  eval_config_lite = tfma.EvalConfig()\n",
        "  eval_config_lite.CopyFrom(eval_config)\n",
        "  # Informs the evaluator that the model is a TFLite model.\n",
        "  eval_config_lite.model_specs[0].model_type = 'tf_lite'\n",
        "\n",
        "  # Uses TFMA to compute the evaluation statistics over features of a model.\n",
        "  evaluator = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer.outputs['model'],\n",
        "      eval_config=eval_config,\n",
        "      instance_name='mnist')\n",
        "\n",
        "  # Uses TFMA to compute the evaluation statistics over features of a TFLite\n",
        "  # model.\n",
        "  evaluator_lite = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer_lite.outputs['model'],\n",
        "      eval_config=eval_config_lite,\n",
        "      instance_name='mnist_lite')\n",
        "\n",
        "  # Checks whether the model passed the validation steps and pushes the model\n",
        "  # to a file destination if check passed.\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)),\n",
        "      instance_name='mnist')\n",
        "\n",
        "  # Checks whether the TFLite model passed the validation steps and pushes the\n",
        "  # model to a file destination if check passed.\n",
        "  pusher_lite = Pusher(\n",
        "      model=trainer_lite.outputs['model'],\n",
        "      model_blessing=evaluator_lite.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir_lite)),\n",
        "      instance_name='mnist_lite')\n",
        "\n",
        "  return pipeline.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      components=[\n",
        "          example_gen,\n",
        "          statistics_gen,\n",
        "          schema_gen,\n",
        "          example_validator,\n",
        "          transform,\n",
        "          trainer,\n",
        "          trainer_lite,\n",
        "          evaluator,\n",
        "          evaluator_lite,\n",
        "          pusher,\n",
        "          pusher_lite,\n",
        "      ],\n",
        "      enable_cache=True,\n",
        "      metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "          metadata_path),\n",
        "      beam_pipeline_args=beam_pipeline_args)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y-d8dR8aQaWh",
        "outputId": "32f66b56-46b4-479e-9195-9c11cb8e9aa9"
      },
      "source": [
        "# To run this pipeline from the python CLI:\n",
        "#   $python mnist_pipeline_native_keras.py\n",
        "if __name__ == '__main__':\n",
        "  absl.logging.set_verbosity(absl.logging.INFO)\n",
        "  BeamDagRunner().run(\n",
        "      _create_pipeline(\n",
        "          pipeline_name=_pipeline_name,\n",
        "          pipeline_root=_pipeline_root,\n",
        "          data_root=_data_root,\n",
        "          module_file=_module_file,\n",
        "          module_file_lite=_module_file_lite,\n",
        "          serving_model_dir=_serving_model_dir,\n",
        "          serving_model_dir_lite=_serving_model_dir_lite,\n",
        "          metadata_path=_metadata_path,\n",
        "          beam_pipeline_args=_beam_pipeline_args))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-e6003c343fad>:7: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "external_input is deprecated, directly pass the uri to ExampleGen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-e6003c343fad>:7: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "external_input is deprecated, directly pass the uri to ExampleGen.\n",
            "WARNING:absl:The \"input\" argument to the ImportExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        var import_html = () => {\n",
              "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
              "            var link = document.createElement('link');\n",
              "            link.rel = 'import'\n",
              "            link.href = href;\n",
              "            document.head.appendChild(link);\n",
              "          });\n",
              "        }\n",
              "        if ('import' in document.createElement('link')) {\n",
              "          import_html();\n",
              "        } else {\n",
              "          var webcomponentScript = document.createElement('script');\n",
              "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
              "          webcomponentScript.type = 'text/javascript';\n",
              "          webcomponentScript.onload = function(){\n",
              "            import_html();\n",
              "          };\n",
              "          document.head.appendChild(webcomponentScript);\n",
              "        }"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Component ImportExampleGen depends on [].\n",
            "INFO:absl:Component ImportExampleGen is scheduled.\n",
            "INFO:absl:Component StatisticsGen depends on ['Run[ImportExampleGen]'].\n",
            "INFO:absl:Component StatisticsGen is scheduled.\n",
            "INFO:absl:Component SchemaGen depends on ['Run[StatisticsGen]'].\n",
            "INFO:absl:Component SchemaGen is scheduled.\n",
            "INFO:absl:Component ExampleValidator depends on ['Run[StatisticsGen]', 'Run[SchemaGen]'].\n",
            "INFO:absl:Component ExampleValidator is scheduled.\n",
            "INFO:absl:Component Transform depends on ['Run[ImportExampleGen]', 'Run[SchemaGen]'].\n",
            "INFO:absl:Component Transform is scheduled.\n",
            "INFO:absl:Component Trainer.mnist depends on ['Run[Transform]', 'Run[SchemaGen]'].\n",
            "INFO:absl:Component Trainer.mnist is scheduled.\n",
            "INFO:absl:Component Trainer.mnist_lite depends on ['Run[Transform]', 'Run[SchemaGen]'].\n",
            "INFO:absl:Component Trainer.mnist_lite is scheduled.\n",
            "INFO:absl:Component Evaluator.mnist depends on ['Run[Trainer.mnist]', 'Run[ImportExampleGen]'].\n",
            "INFO:absl:Component Evaluator.mnist is scheduled.\n",
            "INFO:absl:Component Evaluator.mnist_lite depends on ['Run[Trainer.mnist_lite]', 'Run[ImportExampleGen]'].\n",
            "INFO:absl:Component Evaluator.mnist_lite is scheduled.\n",
            "INFO:absl:Component Pusher.mnist depends on ['Run[Trainer.mnist]', 'Run[Evaluator.mnist]'].\n",
            "INFO:absl:Component Pusher.mnist is scheduled.\n",
            "INFO:absl:Component Pusher.mnist_lite depends on ['Run[Trainer.mnist_lite]', 'Run[Evaluator.mnist_lite]'].\n",
            "INFO:absl:Component Pusher.mnist_lite is scheduled.\n",
            "INFO:absl:Component ImportExampleGen is running.\n",
            "INFO:absl:Running driver for ImportExampleGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:select span and version = (0, None)\n",
            "INFO:absl:latest span and version = (0, None)\n",
            "INFO:absl:Running executor for ImportExampleGen\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmpi3jqw82w/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpi3jqw82w/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpi3jqw82w/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpi3jqw82w/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Generating examples.\n",
            "INFO:absl:Reading input TFRecord data /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/examples/mnist/data/*.\n",
            "INFO:absl:Examples generated.\n",
            "INFO:absl:Running publisher for ImportExampleGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component ImportExampleGen is finished.\n",
            "INFO:absl:Component StatisticsGen is running.\n",
            "INFO:absl:Running driver for StatisticsGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for StatisticsGen\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmp9l4yjn5_/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmp9l4yjn5_/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmp9l4yjn5_/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmp9l4yjn5_/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Generating statistics for split train.\n",
            "INFO:absl:Statistics for split train written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/StatisticsGen/statistics/9/train.\n",
            "INFO:absl:Generating statistics for split eval.\n",
            "INFO:absl:Statistics for split eval written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/StatisticsGen/statistics/9/eval.\n",
            "INFO:absl:Running publisher for StatisticsGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component StatisticsGen is finished.\n",
            "INFO:absl:Component SchemaGen is running.\n",
            "INFO:absl:Running driver for SchemaGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for SchemaGen\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmpq3dvm3ov/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpq3dvm3ov/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpq3dvm3ov/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpq3dvm3ov/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Processing schema from statistics for split train.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:absl:Processing schema from statistics for split eval.\n",
            "INFO:absl:Schema written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/SchemaGen/schema/10/schema.pbtxt.\n",
            "INFO:absl:Running publisher for SchemaGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component SchemaGen is finished.\n",
            "INFO:absl:Component Transform is running.\n",
            "INFO:absl:Running driver for Transform\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Transform\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmp4nugb1bs/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmp4nugb1bs/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmp4nugb1bs/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmp4nugb1bs/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tfx/components/transform/executor.py:528: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tfx/components/transform/executor.py:528: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_transform/tf_utils.py:250: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_transform/tf_utils.py:250: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n",
            "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
            "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Transform/transform_graph/11/.temp_path/tftransform_tmp/a87ceaed910f45fc81975bc5824ede32/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Transform/transform_graph/11/.temp_path/tftransform_tmp/a87ceaed910f45fc81975bc5824ede32/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Transform/transform_graph/11/.temp_path/tftransform_tmp/c695399d191445ecbf16e42b75d32869/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Transform/transform_graph/11/.temp_path/tftransform_tmp/c695399d191445ecbf16e42b75d32869/saved_model.pb\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "INFO:absl:Feature image_class has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
            "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
            "INFO:absl:Running publisher for Transform\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Transform is finished.\n",
            "INFO:absl:Component Trainer.mnist is running.\n",
            "INFO:absl:Running driver for Trainer.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Trainer.mnist\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmpi7ebu4dd/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpi7ebu4dd/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpi7ebu4dd/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpi7ebu4dd/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
            "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "INFO:absl:Training model.\n",
            "INFO:absl:Feature image_class_xf has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats_xf has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class_xf has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats_xf has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:absl:Model: \"sequential\"\n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:Layer (type)                 Output Shape              Param #   \n",
            "INFO:absl:=================================================================\n",
            "INFO:absl:dense (Dense)                (None, 64)                50240     \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dropout (Dropout)            (None, 64)                0         \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dense_1 (Dense)              (None, 64)                4160      \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dropout_1 (Dropout)          (None, 64)                0         \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dense_2 (Dense)              (None, 10)                650       \n",
            "INFO:absl:=================================================================\n",
            "INFO:absl:Total params: 55,050\n",
            "INFO:absl:Trainable params: 55,050\n",
            "INFO:absl:Non-trainable params: 0\n",
            "INFO:absl:_________________________________________________________________\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r   1/5000 [..............................] - ETA: 1s - loss: 2.4015 - sparse_categorical_accuracy: 0.0500WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   2/5000 [..............................] - ETA: 9:21 - loss: 2.5703 - sparse_categorical_accuracy: 0.0750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.2156s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.2156s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - ETA: 0s - loss: 0.0967 - sparse_categorical_accuracy: 0.9677INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5000/5000 [==============================] - 23s 5ms/step - loss: 0.0967 - sparse_categorical_accuracy: 0.9677 - val_loss: 1.6516 - val_sparse_categorical_accuracy: 0.8455\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist/model/12/serving_model_dir/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist/model/12/serving_model_dir/assets\n",
            "INFO:absl:Training complete. Model written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist/model/12/serving_model_dir. ModelRun written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist/model_run/12\n",
            "INFO:absl:Running publisher for Trainer.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Trainer.mnist is finished.\n",
            "INFO:absl:Component Evaluator.mnist is running.\n",
            "INFO:absl:Running driver for Evaluator.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Evaluator.mnist\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmp2bfx6v49/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmp2bfx6v49/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmp2bfx6v49/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmp2bfx6v49/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "WARNING:absl:\"maybe_add_baseline\" and \"maybe_remove_baseline\" are deprecated,\n",
            "        please use \"has_baseline\" instead.\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"image_class\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"SparseCategoricalAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:Using /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist/model/12/serving_model_dir as  model.\n",
            "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
            "INFO:absl:Evaluating model.\n",
            "INFO:absl:Evaluation complete. Results written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Evaluator.mnist/evaluation/13.\n",
            "INFO:absl:Checking validation results.\n",
            "INFO:absl:Blessing result True written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Evaluator.mnist/blessing/13.\n",
            "INFO:absl:Running publisher for Evaluator.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Evaluator.mnist is finished.\n",
            "INFO:absl:Component Pusher.mnist is running.\n",
            "INFO:absl:Running driver for Pusher.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Pusher.mnist\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmpurc537uk/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpurc537uk/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpurc537uk/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpurc537uk/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Model version: 1607673141\n",
            "INFO:absl:Model written to serving path /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/examples/mnist/serving_model/mnist_native_keras/1607673141.\n",
            "INFO:absl:Model pushed to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Pusher.mnist/pushed_model/14.\n",
            "INFO:absl:Running publisher for Pusher.mnist\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Pusher.mnist is finished.\n",
            "INFO:absl:Component Trainer.mnist_lite is running.\n",
            "INFO:absl:Running driver for Trainer.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Trainer.mnist_lite\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmpq7a94unm/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpq7a94unm/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpq7a94unm/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpq7a94unm/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
            "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "INFO:absl:Training model.\n",
            "INFO:absl:Feature image_class_xf has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats_xf has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_class_xf has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature image_floats_xf has a shape dim {\n",
            "  size: 784\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:absl:Model: \"sequential_1\"\n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:Layer (type)                 Output Shape              Param #   \n",
            "INFO:absl:=================================================================\n",
            "INFO:absl:dense_3 (Dense)              (None, 64)                50240     \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dropout_2 (Dropout)          (None, 64)                0         \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dense_4 (Dense)              (None, 64)                4160      \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dropout_3 (Dropout)          (None, 64)                0         \n",
            "INFO:absl:_________________________________________________________________\n",
            "INFO:absl:dense_5 (Dense)              (None, 10)                650       \n",
            "INFO:absl:=================================================================\n",
            "INFO:absl:Total params: 55,050\n",
            "INFO:absl:Trainable params: 55,050\n",
            "INFO:absl:Non-trainable params: 0\n",
            "INFO:absl:_________________________________________________________________\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   2/5000 [..............................] - ETA: 5:13 - loss: 2.3388 - sparse_categorical_accuracy: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.1157s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.1157s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 24s 5ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9674 - val_loss: 1.6111 - val_sparse_categorical_accuracy: 0.8403\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist_lite/model/15/serving_model_dir/temp/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist_lite/model/15/serving_model_dir/temp/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f855aef2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f855aef2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f84fc041950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f84fc041950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "INFO:absl:Training complete. Model written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist_lite/model/15/serving_model_dir. ModelRun written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist_lite/model_run/15\n",
            "INFO:absl:Running publisher for Trainer.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Trainer.mnist_lite is finished.\n",
            "INFO:absl:Component Evaluator.mnist_lite is running.\n",
            "INFO:absl:Running driver for Evaluator.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Evaluator.mnist_lite\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmp2nvmc__a/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmp2nvmc__a/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmp2nvmc__a/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmp2nvmc__a/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "WARNING:absl:\"maybe_add_baseline\" and \"maybe_remove_baseline\" are deprecated,\n",
            "        please use \"has_baseline\" instead.\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"image_class\"\n",
            "  model_type: \"tf_lite\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"SparseCategoricalAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:Using /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Trainer.mnist_lite/model/15/serving_model_dir as  model.\n",
            "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
            "INFO:absl:Evaluating model.\n",
            "INFO:absl:Evaluation complete. Results written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Evaluator.mnist_lite/evaluation/16.\n",
            "INFO:absl:Checking validation results.\n",
            "INFO:absl:Blessing result True written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Evaluator.mnist_lite/blessing/16.\n",
            "INFO:absl:Running publisher for Evaluator.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Evaluator.mnist_lite is finished.\n",
            "INFO:absl:Component Pusher.mnist_lite is running.\n",
            "INFO:absl:Running driver for Pusher.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for Pusher.mnist_lite\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmprr6c4ez2/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmprr6c4ez2/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmprr6c4ez2/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmprr6c4ez2/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Model version: 1607673184\n",
            "INFO:absl:Model written to serving path /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/examples/mnist/serving_model_lite/mnist_native_keras/1607673184.\n",
            "INFO:absl:Model pushed to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/Pusher.mnist_lite/pushed_model/17.\n",
            "INFO:absl:Running publisher for Pusher.mnist_lite\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Pusher.mnist_lite is finished.\n",
            "INFO:absl:Component ExampleValidator is running.\n",
            "INFO:absl:Running driver for ExampleValidator\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Running executor for ExampleValidator\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.6/dist-packages/tfx to temp dir /tmp/tmp3ngs6qxv/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmp3ngs6qxv/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmp3ngs6qxv/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmp3ngs6qxv/build/tfx/dist/tfx_ephemeral-0.25.0.tar.gz to beam args\n",
            "INFO:absl:Validating schema against the computed statistics for split train.\n",
            "INFO:absl:Validation complete for split train. Anomalies written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/ExampleValidator/anomalies/18/train.\n",
            "INFO:absl:Validating schema against the computed statistics for split eval.\n",
            "INFO:absl:Validation complete for split eval. Anomalies written to /content/drive/MyDrive/Fall_2020/Advanced Deep Learning- CMPE 297- 49/Assignments/Assignment 6_TFX interactivecontext based colab/tfx/pipelines/mnist_native_keras/ExampleValidator/anomalies/18/eval.\n",
            "INFO:absl:Running publisher for ExampleValidator\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component ExampleValidator is finished.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}